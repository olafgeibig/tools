# LiteLLM Model Configuration
# Copy to litellm.yaml and customize
# This file defines the models available through the proxy

# Global LiteLLM settings
litellm_settings:
  # Drop unsupported parameters before sending to provider
  drop_params: true
  
  # Enable caching for better performance
  cache: true
  cache_params:
    type: local
    # For Redis cache, uncomment and configure:
    # type: redis
    # host: localhost
    # port: 6379

# List of models available through the proxy
model_list:
  # Example Azure OpenAI model
  - model_name: gpt-5
    litellm_params:
      model: azure/gpt-5
      api_base: https://myazureenv.openai.azure.com
      api_version: 2025-04-01-preview
      # Use environment variable for API key (recommended)
      api_key: "os.environ/API_KEY"
      # Parameters to drop before sending to provider
      additional_drop_params: ["max_tokens", "temperature", "reasoningEffort", "textVerbosity"]
    model_info:
      base_model: azure/gpt-5
      # Add model metadata
      # mode: "chat"
      # supports_function_calling: true
      # supports_vision: true

  # Example OpenAI model
  # - model_name: gpt-4
  #   litellm_params:
  #     model: openai/gpt-4
  #     api_key: "os.environ/OPENAI_API_KEY"
  #   model_info:
  #     base_model: gpt-4

  # Example Anthropic model
  # - model_name: claude-3-sonnet
  #   litellm_params:
  #     model: anthropic/claude-3-sonnet-20240229
  #     api_key: "os.environ/ANTHROPIC_API_KEY"
  #   model_info:
  #     base_model: claude-3-sonnet-20240229

  # Example Google model
  # - model_name: gemini-pro
  #   litellm_params:
  #     model: google/gemini-pro
  #     api_key: "os.environ/GOOGLE_API_KEY"
  #   model_info:
  #     base_model: gemini-pro

# General settings for the proxy
general_settings:
  # Database for storing request logs and usage
  database_url: "sqlite:///./litellm.db"
  
  # For PostgreSQL, uncomment and configure:
  # database_url: "postgresql://user:password@localhost:5432/litellm"
  
  # Enable request logging
  master_key: null  # Set to a secure key for authentication
  
  # Rate limiting (optional)
  # rpm_limit: 100  # Requests per minute per user
  
  # Additional settings
  # set_verbose: false
  # success_callback: ["langfuse"]  # For observability